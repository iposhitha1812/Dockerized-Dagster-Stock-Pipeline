# ğŸš€ Dockerized Stock Market Data Pipeline using Dagster

A **fully containerized ETL pipeline** that fetches, processes, and stores stock market data using **Dagster**, **Python**, **PostgreSQL**, and **Docker**.

This project demonstrates:

1. Workflow orchestration with **Dagster**  
2. Containerization using **Docker & Docker Compose**  
3. Automated stock data ingestion from an external API  
4. Data cleaning and insertion into **PostgreSQL**  
5. End-to-end **ETL pipeline** execution in containers  

## âœ¨ Features

- âœ… Fetches **daily stock prices** from AlphaVantage API  
- âœ… Parses **JSON â†’ Structured rows**  
- âœ… Inserts cleaned data into **PostgreSQL**  
- âœ… Supports **scheduled runs** via Dagster Daemon  
- âœ… Fully **Dockerized** for easy setup & reproducibility  
- âœ… Includes **SQL schema** for automatic table creation  
- âœ… Easily extendable for multiple stock symbols
  

## ğŸ“ Project Structure

```
dockerized-dagster-stock-pipeline/
â”‚â”€â”€ fetcher/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ fetch_and_store.py
â”‚
â”‚â”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ stock_pipeline.py
â”‚
â”‚â”€â”€ sql/
â”‚   â””â”€â”€ create_table.sql
â”‚
â”‚â”€â”€ dagster_repository.py
â”‚â”€â”€ workspace.yaml
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .env
â”‚â”€â”€ README.md
```

## ğŸ”§ Environment Setup

Create a `.env` file in the project root with the following variables:
.env file

```
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=stockdb

ALPHAVANTAGE_API_KEY=YOUR_API_KEY_HERE
STOCK_SYMBOL=MSFT
```

## ğŸ³ Run the Project

### 1ï¸âƒ£ Start All Services

Build and start the containers:

     docker compose up --build

This launches:
- Dagster Webserver  
- Dagster Daemon  
- PostgreSQL Database  

### 2ï¸âƒ£ Open Dagster UI

Visit the Dagster UI at  

ğŸ‘‰ [http://localhost:3000](http://localhost:3000)  

You should see the repository and pipeline listed.

### 3ï¸âƒ£ Trigger a Pipeline Run

In the Dagster UI:

         stock_pipeline â†’ Launch Run

This will start the ETL process to fetch and store stock data.

### 4ï¸âƒ£ View Data in PostgreSQL

Open a terminal and connect to the database:

                 docker exec -it dockerized-dagster-stock-pipeline-postgres-1 bash psql -U postgres -d stockdb
                 
Run a sample query:

                SELECT * FROM stock_data LIMIT 20;
                
### ğŸ›  Stop All Services
To stop and remove containers, run:

               docker compose down
               

## ğŸ“š Technology Stack

| Component | Purpose |
|------------|----------|
| **Dagster** | Workflow orchestration & scheduling |
| **Python** | Fetching & parsing stock data |
| **PostgreSQL** | Persistent data storage |
| **Docker Compose** | Multi-service containerization |
| **AlphaVantage API** | Stock market data source |


## ğŸŒŸ Future Improvements

Potential enhancements for later versions:
1. Support for multiple stock symbols  
2. Enhanced logging & monitoring  
3. Grafana dashboard for stock trend visualization  
4. Apache Airflow version for comparison  

![Dagster Pipeline Overview](image.png)

